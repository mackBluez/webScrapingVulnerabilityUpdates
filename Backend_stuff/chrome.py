import requests
from bs4 import BeautifulSoup
import csv
import itertools

response = requests.get("https://chromereleases.googleblog.com/")
soup = BeautifulSoup(response.text, "html.parser")
data_chrome = []
table_chrome = soup.findAll('h2', attrs={'class':'title'})
for row in table_chrome:
    cols = row.find_all('a')
    cols = [ele.text.strip() for ele in cols]
    data_chrome.append(set(ele for ele in cols if ele))

dates = []
date_chrome = soup.findAll('span', attrs={'class':'publishdate'})

for date in date_chrome:
    dates.append(date.text.strip("\n"))

data_chrome = list(filter(None, data_chrome))
mapp = zip(data_chrome, dates)
mappp = list(mapp)

for names in mappp:
    with open('C:\\Automation_OOB\\Backend_stuff\\chrome.txt', 'a') as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(names)



print("Chrome Scraped...")

